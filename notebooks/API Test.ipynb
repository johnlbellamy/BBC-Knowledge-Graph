{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 2.08MB/s]                    \n",
      "2020-12-22 15:47:33 INFO: Downloading default packages for language: en (English)...\n",
      "2020-12-22 15:47:34 INFO: File exists: /Users/johnbellamy/stanza_resources/en/default.zip.\n",
      "2020-12-22 15:47:37 INFO: Finished downloading models and saved to /Users/johnbellamy/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    ">>> import stanza\n",
    ">>> stanza.download('en')       # This downloads the English models for the neural pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-22 15:47:37 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | ewt       |\n",
      "| pos       | ewt       |\n",
      "| lemma     | ewt       |\n",
      "| depparse  | ewt       |\n",
      "| sentiment | sstplus   |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2020-12-22 15:47:37 INFO: Use device: cpu\n",
      "2020-12-22 15:47:37 INFO: Loading: tokenize\n",
      "2020-12-22 15:47:37 INFO: Loading: pos\n",
      "2020-12-22 15:47:38 INFO: Loading: lemma\n",
      "2020-12-22 15:47:38 INFO: Loading: depparse\n",
      "2020-12-22 15:47:39 INFO: Loading: sentiment\n",
      "2020-12-22 15:47:40 INFO: Loading: ner\n",
      "2020-12-22 15:47:41 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    ">>> nlp = stanza.Pipeline('en') # This sets up a default neural pipeline in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 990,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(nlp, open( \"nlp.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 963,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Where', 4, 'advmod')\n",
      "('does', 4, 'aux')\n",
      "('GM', 4, 'nsubj')\n",
      "('live', 0, 'root')\n",
      "('?', 4, 'punct')\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Where does GM live?\")\n",
    "doc.sentences[0].print_dependencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 964,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{\n",
       "   \"text\": \"GM\",\n",
       "   \"type\": \"ORG\",\n",
       "   \"start_char\": 11,\n",
       "   \"end_char\": 13\n",
       " }]"
      ]
     },
     "execution_count": 964,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 968,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = []\n",
    "\n",
    "for ent in doc.ents:\n",
    "    \n",
    "    ent = ent.to_dict()\n",
    "    ner.append((ent['text'], ent['type'])) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words = [x for x in stop_words if x != \"do\" and x != \"do?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    " \n",
    "pickle.dump(stop_words, open( \"stops.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from py2neo import Graph\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import stanza\n",
    "\n",
    "from nltk import pos_tag\n",
    "import itertools\n",
    "from itertools import groupby\n",
    "import pickle\n",
    "\n",
    "\n",
    "class NLS:\n",
    "\n",
    "    def __init__(self, question, nlp):\n",
    "\n",
    "        self.question = question\n",
    "        self.__companies__ = pickle.load(open('nls-app/companies.p', 'rb'))\n",
    "        self.__companies__map__ = pickle.load(open('nls-app/companies_map.p', 'rb'))\n",
    "        self.__people_companies_map__ = pickle.load(open('nls-app/people_companies.p', 'rb'))\n",
    "        self.__people_locations_map__ = pickle.load(open('nls-app/people_locations.p', 'rb'))\n",
    "        self.__nlp__ = nlp\n",
    "        self.graph = Graph(\"bolt://localhost:7687\", auth = (\"neo4j\", \"test\"))\n",
    "   \n",
    "\n",
    "    def tokenize(question):\n",
    "    \n",
    "        question_tokenized = word_tokenize(question)\n",
    "        \n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        filtered_question = [w for w in question_tokenized if not w in stop_words]\n",
    "        \n",
    "        filtered_question = []\n",
    "    \n",
    "        for w in question_tokenized:\n",
    "        \n",
    "            if w not in stop_words:\n",
    "            \n",
    "                filtered_question.append(w)\n",
    "            \n",
    "        return filtered_question\n",
    "\n",
    "    def tag(self):\n",
    "    \n",
    "        ner, tags = [], []\n",
    "        \n",
    "        self.question = self.question.replace(\"'s\",\"\")\n",
    "        \n",
    "        if self.question[-1] == 's':\n",
    "            \n",
    "            self.question[-1] == ''\n",
    "            \n",
    "            \n",
    "        for pair in itertools.combinations(self.question.split(), 2):\n",
    "            \n",
    "            pair = ' '.join(pair)\n",
    "            \n",
    "            people1 =  self.__people_companies_map__.get(pair)\n",
    "            people2 =  self.__people_locations_map__.get(pair)\n",
    "            \n",
    "            if people1:\n",
    "                \n",
    "                self.question = self.question.replace(pair, people1)\n",
    "                \n",
    "            if people2:\n",
    "        \n",
    "                self.question = self.question.replace(pair, people2)\n",
    "                \n",
    "                \n",
    "        doc = self.__nlp__(self.question)\n",
    "                \n",
    "        for ent in doc.ents:\n",
    "\n",
    "            ent = ent.to_dict()\n",
    "            pairs = (ent['text'], ent['type'])\n",
    "            if pairs not in ner:\n",
    "                    \n",
    "                ner.append(pairs)\n",
    "            \n",
    "        #displacy.render(doc)\n",
    "        \n",
    "        self.filtered_question = NLS.tokenize(self.question)\n",
    "        \n",
    "        tags = pos_tag(self.filtered_question)\n",
    "        \n",
    "        groups = groupby(tags, key=lambda x: x[1])\n",
    "        \n",
    "        names_tagged = [[w for w,_ in words] for tag,words in groups if tag == \"NNP\"]\n",
    "        \n",
    "        names = [\" \".join(name) for name in names_tagged if len(name) >= 2]\n",
    "        \n",
    "        if len(ner) == 0:\n",
    "            \n",
    "            if any([x in self.__companies__ for x in self.filtered_question]):\n",
    "                \n",
    "                matches = [x for x in self.__companies__ if x in self.filtered_question]\n",
    "                \n",
    "                for m in matches:\n",
    "                    \n",
    "                    ner.append(( self.__companies__map__[m], \"ORG\"))\n",
    "                \n",
    "        self.ner = ner\n",
    "        self.tags = tags \n",
    "\n",
    "    def params_builder(self):\n",
    "    \n",
    "        params, params_2 = {}, {}\n",
    "        \n",
    "        if len(self.ner) == 1:\n",
    "            \n",
    "            if (self.ner[0][1] == 'GPE') or (self.ner[0][1] == 'LOC'):\n",
    "                \n",
    "                if (self.ner[0][0] == \"US\") or (self.ner[0][0] == \"USA\"):\n",
    "                    \n",
    "                    country_ = 'United States'\n",
    "                    \n",
    "                elif (self.ner[0][0] == \"UK\"):\n",
    "                    \n",
    "                    country_ = 'United Kingdom'\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    country = self.ner[0][0]\n",
    "                    \n",
    "                    params = {}\n",
    "                    \n",
    "                    params[\"country\"] = country\n",
    "                    \n",
    "            elif (self.ner[0][1] == 'ORG'):\n",
    "                \n",
    "                org = self.ner[0][0]\n",
    "                \n",
    "                params = {}\n",
    "                params[\"org\"] = org                    \n",
    "                \n",
    "            elif (self.ner[0][1] == 'PERSON'):\n",
    "                \n",
    "                person = self.ner[0][0]\n",
    "                \n",
    "                params = {}\n",
    "                \n",
    "                params[\"person\"] = person\n",
    "                \n",
    "        elif len(self.ner) > 1:\n",
    "            \n",
    "            name1 = self.ner[0][0]\n",
    "            \n",
    "            name2 = self.ner[1][0]\n",
    "            \n",
    "            params_2 = {\"name1\":name1, \"name2\":name2}\n",
    "            \n",
    "        self.params = params \n",
    "        self.params_2 = params_2\n",
    "\n",
    "    def query_picker(self):\n",
    "    \n",
    "        label_p = \"(p:Person)\"\n",
    "        label_o = \"(o:Company)\"\n",
    "        label_l = \"(l:NE_Location)\"\n",
    "        works = \"-[r:WORKS_FOR]-\"\n",
    "        lives = \"-[:LIVES_IN]-\"\n",
    "        \n",
    "        default =\"\"\"match (a:Article)\n",
    "        return a.Text\n",
    "        limit 10\"\"\"\n",
    "        \n",
    "        default_unkown =\"\"\"match (a:Article)\n",
    "        where \n",
    "        return a.Text\n",
    "        limit 10\"\"\"\n",
    "\n",
    "        query1 = '''\n",
    "        match {} {} {}\n",
    "        where p.value = $person\n",
    "        return p.value as Name,o.value as WORKS_FOR\n",
    "        '''.format(label_p,works,label_o)\n",
    "\n",
    "        query1_1 = '''\n",
    "        match {} {} {}\n",
    "        where o.value = $org\n",
    "        return o.value as Organization, collect(distinct p.value) as Person, r.value as Position\n",
    "        '''.format(label_p,works,label_o)\n",
    "\n",
    "        query2 = '''\n",
    "        match (p:Person {value:$person})-[r:LIVES_IN]-(l:Location)\n",
    "        return p.value as Person, l.value as STAYS_AT\n",
    "        '''\n",
    "\n",
    "        query3 = '''\n",
    "        MATCH (p1:NER_Person:Tag{ value: $name1 }),(p2:NER_Person:Tag{ value: $name2 }), p = shortestPath((p1)-[*..15]-(p2))\n",
    "        RETURN p1.value as Person1, p2.value as Person2, p as Relation\n",
    "        '''\n",
    "\n",
    "        query4 = '''\n",
    "        MATCH (p:NER_Person)-[w:LIVES_IN]->(o:NER_Location)\n",
    "        RETURN p.value as Person ,o.value as Lives_in\n",
    "        '''.format(label_p,lives,label_o)\n",
    "\n",
    "        query5 = '''\n",
    "        MATCH (p:NER_Person)-[:LIVES_IN]->(l:NER_Location), (p)-[w:WORKS_AT]-(o:NER_Organization)\n",
    "        RETURN p.value as Person, l.value as Lives_in, o.value as Works_at, w.AS as Position\n",
    "        '''\n",
    "\n",
    "        query6 = '''\n",
    "        MATCH (s:Sentence)-[st:SENTENCE_TAG_OCCURRENCE]->(n:TagOccurrence), (s)-[h:HAS_TAG]-(p:NER_Person), (s)-[h]-(o:NER_Organization)\n",
    "        where n.value IN [\"said\",\"says\",\"think\",\"thinks\"] AND (p.value in $names OR o.value in $org)\n",
    "        return s.text as Sentence, p.value as Person\n",
    "        '''\n",
    "\n",
    "        query7 = '''\n",
    "        match (a:Article)-[:HAS_ANNOTATED_TEXT]-(at:AnnotatedText)-[:CONTAINS_SENTENCE]-(s:Sentence)-[:SENTENCE_TAG_OCCURRENCE]-(t:TagOccurrence)-[:TAG_OCCURRENCE_TAG]-(o:NER_Organization)\n",
    "        where o.value = $org\n",
    "        return distinct a.Text\n",
    "\n",
    "        '''\n",
    "\n",
    "        query8 = '''\n",
    "        match (a:Article)-[:HAS_ANNOTATED_TEXT]-(at:AnnotatedText)-[:CONTAINS_SENTENCE]-(s:Sentence)-[:SENTENCE_TAG_OCCURRENCE]-(t:TagOccurrence)-[:TAG_OCCURRENCE_TAG]-(p:NER_Person)\n",
    "        where p.value = $person\n",
    "        return distinct a.Text\n",
    "\n",
    "        '''\n",
    "        \n",
    "        query9 = '''\n",
    "        match (p:Person {value: $person})-[:HAS_TITLE]-(t:Title)\n",
    "        return distinct p.value as Person, t.value as HAS_TITLE\n",
    "        '''\n",
    "        \n",
    "        tags_list = [x[0] for x in self.tags]\n",
    "        pos_tag_list = [x[1] for x in self.tags]\n",
    "        value_list = []\n",
    "\n",
    "        if not self.tags and not self.params:\n",
    "            \n",
    "            self.response = [self.graph.run(default).data(), \"article\", \"null\"]\n",
    "            \n",
    "        if self.tags and self.params and not all(e == 'NNP' for e in pos_tag_list) and not all(e == 'NN' for e in pos_tag_list):\n",
    "            \n",
    "            for _, value in self.params.items():\n",
    "                \n",
    "                for v in value.split():\n",
    "                    \n",
    "                    if v not in value_list:\n",
    "                        \n",
    "                        value_list.append(v)\n",
    "\n",
    "            if all(value_list for a in tags_list):\n",
    "\n",
    "                non_param_tags = [x for x in tags_list if x not in value_list]\n",
    "\n",
    "                for word in non_param_tags:\n",
    "\n",
    "\n",
    "                    if word in ['work','do']: # look verbs and see if there is a work verb \n",
    "\n",
    "                        self.response = [self.graph.run(query1, self.params).data(), \"rdf-triple\"]\n",
    "\n",
    "                    elif word in ['works','at']:\n",
    "\n",
    "                        self.response = [self.graph.run(query1_1, self.params).data(), \"rdf-triple\"]\n",
    "                    \n",
    "                    elif word in ['title','job','role','position', 'job?']:\n",
    "\n",
    "                        self.response = [self.graph.run(query9, self.params).data(), \"rdf-triple\"]\n",
    "\n",
    "                    elif word in ['live','reside','stay', 'live?']:\n",
    "                        \n",
    "                        print(\"live\")\n",
    "                        \n",
    "                        try:\n",
    "                            \n",
    "                            if len(self.params.get('person').split()) == 1:\n",
    "                            \n",
    "                                words = [x[0][0].upper() + x[0][1:] for x in self.tags if x[1] == \"NNP\" or x[1] == \"NN\"] \n",
    "                \n",
    "                                params = {\"person\":\" \".join(words)}\n",
    "                    \n",
    "                            else:\n",
    "                            \n",
    "                                params = self.params\n",
    "                            \n",
    "                        except:\n",
    "                            \n",
    "                            if len(self.params.get('org').split()) == 1:\n",
    "                            \n",
    "                                words = [x[0][0].upper() + x[0][1:] for x in self.tags if x[1] == \"NNP\" or x[1] == \"NN\"] \n",
    "                \n",
    "                                params = {\"person\":\" \".join(words)}\n",
    "                         \n",
    "                    \n",
    "                            else:\n",
    "                            \n",
    "                                params = self.params\n",
    "                                \n",
    "                     \n",
    "\n",
    "                        self.response = [self.graph.run(query2, params).data(), \"rdf-triple\"]\n",
    "\n",
    "                    elif word in ['related','relation']:\n",
    "\n",
    "                        self.response = self.graph.run(query3, self.params_2).data()\n",
    "\n",
    "\n",
    "                    elif word in ['everyone']:\n",
    "\n",
    "                        self.response = self.graph.run(query5).data()\n",
    "\n",
    "                    elif word in ['think', 'says', 'say']:\n",
    "\n",
    "                        self.response = self.graph.run(query6, self.params).data() \n",
    "                        \n",
    "                    elif word in ['article','articles', 'Article','Articles']:\n",
    "                        \n",
    "\n",
    "                        if self.params.get('org'):\n",
    "                            \n",
    "\n",
    "                            self.response = [self.graph.run(query7, self.params).data(), \"article\", self.params['org']]\n",
    "\n",
    "                        else:\n",
    "                            print(\"else\")\n",
    "\n",
    "                            words = [x[0][0].upper() + x[0][1:] for x in self.tags if x[1] == \"NNP\"] \n",
    "\n",
    "                            params = {\"person\":\" \".join(words)}\n",
    "                            self.response = [self.graph.run(query8, params).data(), \"article\", params['person']]\n",
    "                            \n",
    "        \n",
    "        else:\n",
    "            \n",
    "            print(\"else\")\n",
    "            \n",
    "            if self.params.get('person'):\n",
    "                print('person')\n",
    "                \n",
    "                words = [x[0][0].upper() + x[0][1:] for x in self.tags if x[1] == \"NNP\" or x[1] == \"NN\"] \n",
    "                print(words)\n",
    "                \n",
    "                params = {\"person\":\" \".join(words)}\n",
    "                        \n",
    "                self.response = [self.graph.run(query8, params).data(), \"article\", params['person']]\n",
    "                \n",
    "            if self.params.get('org'):\n",
    "                print(\"two\")  \n",
    "                self.response = [self.graph.run(query7, self.params).data(), \"article\", self.params['org']]\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                term = self.tags[0][0][0].upper() + self.tags[0][0][1:]\n",
    "                response1 = self.graph.run(query7, {'org':term}).data()\n",
    "                response2 = self.graph.run(query8, {'person':term}).data()\n",
    "                \n",
    "                if response1:\n",
    "                    \n",
    "                    self.response = [response1, \"article\", term]\n",
    "                    \n",
    "                elif response2:\n",
    "                    \n",
    "                    self.response = [response2, \"article\", term]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fiat'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nls.__companies__map__.get('fiat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nls = NLS(\"what does xie xuren do?\", nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emmanuel gaillard']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[person for person in nls.__people_companies_map__.keys() if person in nls.__people_locations_map__.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "nls.tag()\n",
    "nls.params_builder()\n",
    "nls.query_picker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NLS' object has no attribute 'response'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-60187d25b803>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NLS' object has no attribute 'response'"
     ]
    }
   ],
   "source": [
    "nls.response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    \n",
    "    if nls.response[1] == \"article\":\n",
    "    \n",
    "        articles = []\n",
    "    \n",
    "        for article in nls.response[0]:\n",
    "           \n",
    "            articles.append(article)\n",
    "        \n",
    "        results = {\"responseType\": \"article\", \"entity\": nls.response[2], \"articles\":articles}\n",
    "    \n",
    "    if nls.response[1] == \"rdf-triple\":\n",
    "\n",
    "        try:\n",
    "\n",
    "            results = {'responseType': \"rdf-triple\",\n",
    "                   'subject': nls.response[0][0][list(dict(nls.response[0][0]).keys())[0]],\n",
    "                   'object': nls.response[0][0][list(dict(nls.response[0][0]).keys())[1]],\n",
    "\n",
    "                   'predicate': list(dict(nls.response[0][0]).keys())[1]}\n",
    "\n",
    "            if type(results.get('object')) == list:\n",
    "\n",
    "                results['object'] = \" & \".join(results.get('object'))\n",
    "\n",
    "        except IndexError:\n",
    "\n",
    "            results =  {'responseType': \"rdf-triple\",\n",
    "                   'subject': \"\",\n",
    "                   'object': \"\",\n",
    "                   'predicate': \"\"}\n",
    "            \n",
    "except:\n",
    "    \n",
    "    results =  {'responseType': \"rdf-triple\",\n",
    "                   'subject': \"\",\n",
    "                   'object': \"\",\n",
    "                   'predicate': \"\"}\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'responseType': 'rdf-triple',\n",
       " 'subject': 'Xie Xuren',\n",
       " 'object': 'head',\n",
       " 'predicate': 'HAS_TITLE'}"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Louisa', 'Lim', 'live']"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nls.filtered_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1546,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1546-99b493aabad4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "nls.response(list(dict(nls.response[0][0]).keys())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nls.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 596,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"hello\"[0].isupper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello'"
      ]
     },
     "execution_count": 620,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"hello\"[0][0].upper() + \"hello\"[1:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner, tags = [], []\n",
    "question = \"where does Nestor Kirchner live?\"        \n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "doc = nlp(question)\n",
    "\n",
    "ner = [(X.text, X.label_) for X in doc.ents]\n",
    "\n",
    "#displacy.render(doc)\n",
    "\n",
    "tags = pos_tag(words)\n",
    "\n",
    "groups = groupby(tags, key=lambda x: x[1])\n",
    "\n",
    "names_tagged = [[w for w,_ in words] for tag,words in groups if tag==\"NNP\"]\n",
    "\n",
    "names = [\" \".join(name) for name in names_tagged if len(name)>=2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[' '], [' ', 'N'], [' ', 'K', 'i'], [' ']]"
      ]
     },
     "execution_count": 808,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Nestor Kirchner', 'PERSON')]"
      ]
     },
     "execution_count": 810,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'where does Nestor Kirchner live?'"
      ]
     },
     "execution_count": 811,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'person': 'Kirchner'}"
      ]
     },
     "execution_count": 817,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nls.params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Test.ipynb\r\n",
      "CXFBISOLR.pem\r\n",
      "Entity Linking.ipynb\r\n",
      "Knowledge Graphs.pdf\r\n",
      "Load Data.ipynb\r\n",
      "NLS Notes.docx\r\n",
      "\u001b[34mNeo4j-NLP\u001b[m\u001b[m\r\n",
      "Notes about NLS.docx\r\n",
      "Query Test.ipynb\r\n",
      "UIMA Solr Notes.docx\r\n",
      "USPTO IAISS Trademark Image Search & ID RFI - Octo - 11022020.docx\r\n",
      "\u001b[34m__pycache__\u001b[m\u001b[m\r\n",
      "article_response.json\r\n",
      "\u001b[34mbbc\u001b[m\u001b[m\r\n",
      "\u001b[34mbuild\u001b[m\u001b[m\r\n",
      "cities1000.txt\r\n",
      "cypher_queries.cypher\r\n",
      "dwh_evidence.csv\r\n",
      "dwh_evidence.txt\r\n",
      "dwh_evidence.zip\r\n",
      "evidence.csv\r\n",
      "evidence_1130.json\r\n",
      "evidence_1207.json\r\n",
      "jobs_1120.json\r\n",
      "jobs_41.json\r\n",
      "monster-jobs.csv\r\n",
      "nginx.conf\r\n",
      "\u001b[34mnls-app\u001b[m\u001b[m\r\n",
      "nls-app.zip\r\n",
      "nls.py\r\n",
      "\u001b[34mnltk_contrib\u001b[m\u001b[m\r\n",
      "queries.json\r\n",
      "rdf.py\r\n",
      "schema.xml\r\n",
      "\u001b[34msemantic-knowledge-graph\u001b[m\u001b[m\r\n",
      "solr notes.docx\r\n",
      "\u001b[34mstatistical-phrase-identifier\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from py2neo import Graph\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import en_core_web_sm\n",
    "from nltk import pos_tag\n",
    "from itertools import groupby\n",
    "import pickle\n",
    "\n",
    "\n",
    "class NLS:\n",
    "\n",
    "    def __init__(self, question):\n",
    "\n",
    "        self.question = question\n",
    "        self.companies = pickle.load(open('nls-app/companies.p', 'rb'))\n",
    "        self.companies_map = pickle.load(open('nls-app/companies_map.p', 'rb'))\n",
    "        self.people_companies_map = pickle.load(open('nls-app/people_companies.p', 'rb'))\n",
    "        self.people_locations_map = pickle.load(open('nls-app/people_locations.p', 'rb'))\n",
    "\n",
    "\n",
    "    def connect():\n",
    "        global graph\n",
    "        graph = Graph(\"bolt://ec2-18-191-135-163.us-east-2.compute.amazonaws.com:7687\", auth = (\"neo4j\", \"test\"))\n",
    "        tx = graph.begin()\n",
    "    \n",
    "    def tokenize(self):\n",
    "    \n",
    "        question_tokenized = word_tokenize(self.question)\n",
    "        \n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        filtered_question = [w for w in question_tokenized if not w in stop_words]\n",
    "        \n",
    "        filtered_question = []\n",
    "    \n",
    "        for w in question_tokenized:\n",
    "        \n",
    "            if w not in stop_words:\n",
    "            \n",
    "                filtered_question.append(w)\n",
    "            \n",
    "        self.filtered_question = filtered_question\n",
    "\n",
    "    def tag(self):\n",
    "    \n",
    "        ner, tags = [], []\n",
    "        \n",
    "        nlp = en_core_web_sm.load()\n",
    "        \n",
    "        doc = nlp(self.question)\n",
    "        \n",
    "        ner = [(X.text, X.label_) for X in doc.ents]\n",
    "            \n",
    "        #displacy.render(doc)\n",
    "        \n",
    "        tags = pos_tag(self.filtered_question)\n",
    "        \n",
    "        groups = groupby(tags, key=lambda x: x[1])\n",
    "        \n",
    "        names_tagged = [[w for w,_ in words] for tag,words in groups if tag==\"NNP\"]\n",
    "        \n",
    "        names = [\" \".join(name) for name in names_tagged if len(name)>=2]\n",
    "        \n",
    "        if len(ner) == 0:\n",
    "            \n",
    "            if any([x in self.companies for x in self.filtered_question]):\n",
    "                \n",
    "                matches = [x for x in self.companies if x in self.filtered_question]\n",
    "                \n",
    "                for m in matches:\n",
    "                    \n",
    "                    ner.append((m, \"ORG\"))\n",
    "                \n",
    "        self.ner = ner\n",
    "        self.tags = tags \n",
    "\n",
    "    def params_builder(self):\n",
    "    \n",
    "        params, params_2 = {}, {}\n",
    "        \n",
    "        if len(self.ner) == 1:\n",
    "            \n",
    "            if (self.ner[0][1] == 'GPE') or (self.ner[0][1] == 'LOC'):\n",
    "                \n",
    "                if (self.ner[0][0] == \"US\") or (self.ner[0][0] == \"USA\"):\n",
    "                    \n",
    "                    country_ = 'United States'\n",
    "                    \n",
    "                elif (self.ner[0][0] == \"UK\"):\n",
    "                    \n",
    "                    country_ = 'United Kingdom'\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    country = self.ner[0][0]\n",
    "                    \n",
    "                    params = {}\n",
    "                    \n",
    "                    params[\"country\"] = country\n",
    "                    \n",
    "            elif (self.ner[0][1] == 'ORG'):\n",
    "                \n",
    "                org = self.ner[0][0]\n",
    "                \n",
    "                params = {}\n",
    "                \n",
    "                try:\n",
    "                    params[\"org\"] = companies_map[org]\n",
    "                \n",
    "                except:\n",
    "                \n",
    "                    if org[0].islower():\n",
    "                    \n",
    "                        params[\"org\"] = org[0].upper() + org[1:]\n",
    "                    \n",
    "                    else:\n",
    "                    \n",
    "                        params[\"org\"] = org                    \n",
    "                \n",
    "            elif (self.ner[0][1] == 'PERSON'):\n",
    "                \n",
    "                person = self.ner[0][0]\n",
    "                \n",
    "                params = {}\n",
    "                \n",
    "                params[\"person\"] = person\n",
    "                \n",
    "        elif len(self.ner) > 1:\n",
    "            \n",
    "            name1 = self.ner[0][0]\n",
    "            \n",
    "            name2 = self.ner[1][0]\n",
    "            \n",
    "            params_2 = {\"name1\":name1, \"name2\":name2}\n",
    "            \n",
    "        self.params = params \n",
    "        self.params_2 = params_2\n",
    "\n",
    "    def query_picker(self):\n",
    "    \n",
    "        label_p = \"(p:Person)\"\n",
    "        label_o = \"(o:Company)\"\n",
    "        label_l = \"(l:NE_Location)\"\n",
    "        works = \"-[r:WORKS_FOR]-\"\n",
    "        lives = \"-[:LIVES_IN]-\"\n",
    "        \n",
    "        default =\"\"\"match (a:Article)\n",
    "        return a.Text\n",
    "        limit 10\"\"\"\n",
    "        \n",
    "        default_unkown =\"\"\"match (a:Article)\n",
    "        where \n",
    "        return a.Text\n",
    "        limit 10\"\"\"\n",
    "\n",
    "        query1 = '''\n",
    "        match {} {} {}\n",
    "        where p.value = $person\n",
    "        return p.value as Name,o.value as WORKS_FOR\n",
    "        '''.format(label_p,works,label_o)\n",
    "\n",
    "        query1_1 = '''\n",
    "        match {} {} {}\n",
    "        where o.value = $org\n",
    "        return o.value as Organization, collect(distinct p.value) as Person, r.value as Position\n",
    "        '''.format(label_p,works,label_o)\n",
    "\n",
    "        query2 = '''\n",
    "        match (p:Person {value:$person})-[r:LIVES_IN]-(l:Location)\n",
    "        return p.value as Person, l.value as STAYS_AT\n",
    "        '''\n",
    "\n",
    "        query3 = '''\n",
    "        MATCH (p1:NER_Person:Tag{ value: $name1 }),(p2:NER_Person:Tag{ value: $name2 }), p = shortestPath((p1)-[*..15]-(p2))\n",
    "        RETURN p1.value as Person1, p2.value as Person2, p as Relation\n",
    "        '''\n",
    "\n",
    "        query4 = '''\n",
    "        MATCH (p:NER_Person)-[w:LIVES_IN]->(o:NER_Location)\n",
    "        RETURN p.value as Person ,o.value as Lives_in\n",
    "        '''.format(label_p,lives,label_o)\n",
    "\n",
    "        query5 = '''\n",
    "        MATCH (p:NER_Person)-[:LIVES_IN]->(l:NER_Location), (p)-[w:WORKS_AT]-(o:NER_Organization)\n",
    "        RETURN p.value as Person, l.value as Lives_in, o.value as Works_at, w.AS as Position\n",
    "        '''\n",
    "\n",
    "        query6 = '''\n",
    "        MATCH (s:Sentence)-[st:SENTENCE_TAG_OCCURRENCE]->(n:TagOccurrence), (s)-[h:HAS_TAG]-(p:NER_Person), (s)-[h]-(o:NER_Organization)\n",
    "        where n.value IN [\"said\",\"says\",\"think\",\"thinks\"] AND (p.value in $names OR o.value in $org)\n",
    "        return s.text as Sentence, p.value as Person\n",
    "        '''\n",
    "\n",
    "        query7 = '''\n",
    "        match (a:Article)-[:HAS_ANNOTATED_TEXT]-(at:AnnotatedText)-[:CONTAINS_SENTENCE]-(s:Sentence)-[:SENTENCE_TAG_OCCURRENCE]-(t:TagOccurrence)-[:TAG_OCCURRENCE_TAG]-(o:NER_Organization)\n",
    "        where o.value = $org\n",
    "        return distinct a.Text\n",
    "\n",
    "        '''\n",
    "\n",
    "        query8 = '''\n",
    "        match (a:Article)-[:HAS_ANNOTATED_TEXT]-(at:AnnotatedText)-[:CONTAINS_SENTENCE]-(s:Sentence)-[:SENTENCE_TAG_OCCURRENCE]-(t:TagOccurrence)-[:TAG_OCCURRENCE_TAG]-(p:NER_Person)\n",
    "        where p.value = $person\n",
    "        return distinct a.Text\n",
    "\n",
    "        '''\n",
    "        \n",
    "        tags_list = [x[0] for x in self.tags]\n",
    "        pos_tag_list = [x[1] for x in self.tags]\n",
    "        value_list = []\n",
    "\n",
    "        if not self.tags and not self.params:\n",
    "            \n",
    "            self.response = [graph.run(default).data(), \"article\", \"null\"]\n",
    "            \n",
    "        if self.tags and self.params and not all(e == 'NNP' for e in pos_tag_list) and not all(e == 'NN' for e in pos_tag_list):\n",
    "            \n",
    "            for _, value in self.params.items():\n",
    "                \n",
    "                for v in value.split():\n",
    "                    \n",
    "                    if v not in value_list:\n",
    "                        \n",
    "                        value_list.append(v)\n",
    "\n",
    "            if all(value_list for a in tags_list):\n",
    "\n",
    "                non_param_tags = [x for x in tags_list if x not in value_list]\n",
    "\n",
    "                for word in non_param_tags:\n",
    "\n",
    "\n",
    "                    if word in ['work','do']:\n",
    "\n",
    "                        self.response = [graph.run(query1, self.params).data(), \"rdf-triple\"]\n",
    "\n",
    "                    elif word in ['works','at']:\n",
    "\n",
    "                        self.response = [graph.run(query1_1, self.params).data(), \"rdf-triple\"]\n",
    "\n",
    "                    elif word in ['live','reside','stay', 'live?']:\n",
    "                        \n",
    "                        print(\"live\")\n",
    "                        \n",
    "                        try:\n",
    "                            \n",
    "                            if len(self.params.get('person').split()) == 1:\n",
    "                            \n",
    "                                words = [x[0][0].upper() + x[0][1:] for x in self.tags if x[1] == \"NNP\" or x[1] == \"NN\"] \n",
    "                \n",
    "                                params = {\"person\":\" \".join(words)}\n",
    "                    \n",
    "                            else:\n",
    "                            \n",
    "                                params = self.params\n",
    "                            \n",
    "                        except:\n",
    "                            \n",
    "                            if len(self.params.get('org').split()) == 1:\n",
    "                            \n",
    "                                words = [x[0][0].upper() + x[0][1:] for x in self.tags if x[1] == \"NNP\" or x[1] == \"NN\"] \n",
    "                \n",
    "                                params = {\"person\":\" \".join(words)}\n",
    "                         \n",
    "                    \n",
    "                            else:\n",
    "                            \n",
    "                                params = self.params\n",
    "                                \n",
    "                        print(\" \".join(words))\n",
    "\n",
    "                        self.response = [graph.run(query2, params).data(), \"rdf-triple\"]\n",
    "\n",
    "                    elif word in ['related','relation']:\n",
    "\n",
    "                        self.response = graph.run(query3, self.params_2).data()\n",
    "\n",
    "\n",
    "                    elif word in ['everyone']:\n",
    "\n",
    "                        self.response = graph.run(query5).data()\n",
    "\n",
    "                    elif word in ['think', 'says', 'say']:\n",
    "\n",
    "                        self.response = graph.run(query6, self.params).data() \n",
    "                        \n",
    "                    elif word in ['article','articles', 'Article','Articles']:\n",
    "                        \n",
    "\n",
    "                        if self.params.get('org'):\n",
    "                            \n",
    "\n",
    "                            self.response = [graph.run(query7, self.params).data(), \"article\"]\n",
    "\n",
    "                        else:\n",
    "                            print(\"else\")\n",
    "\n",
    "                            words = [x[0][0].upper() + x[0][1:] for x in self.tags if x[1] == \"NNP\"] \n",
    "\n",
    "                            params = {\"person\":\" \".join(words)}\n",
    "                            self.response = [graph.run(query8, params).data(), \"article\", params['person']]\n",
    "                            \n",
    "        \n",
    "        else:\n",
    "            \n",
    "            print(\"else\")\n",
    "            \n",
    "            if self.params.get('person'):\n",
    "                print('person')\n",
    "                \n",
    "                words = [x[0][0].upper() + x[0][1:] for x in self.tags if x[1] == \"NNP\" or x[1] == \"NN\"] \n",
    "                print(words)\n",
    "                \n",
    "                params = {\"person\":\" \".join(words)}\n",
    "                        \n",
    "                self.response = [graph.run(query8, params).data(), \"article\", params['person']]\n",
    "                \n",
    "            if self.params.get('org'):\n",
    "                print(\"two\")  \n",
    "                self.response = [graph.run(query7, self.params).data(), \"article\", self.params['org']]\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                term = self.tags[0][0][0].upper() + self.tags[0][0][1:]\n",
    "                response1 = graph.run(query7, {'org':term}).data()\n",
    "                response2 = graph.run(query8, {'person':term}).data()\n",
    "                \n",
    "                if response1:\n",
    "                    \n",
    "                    self.response = [response1, \"article\", term]\n",
    "                    \n",
    "                elif response2:\n",
    "                    \n",
    "                    \n",
    "                    self.response = [response2, \"article\", term]\n",
    "                    \n",
    "    \n",
    "                \n",
    "                \n",
    "                \n",
    "            \n",
    "            \n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
